{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01abdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "import librosa as lb\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os, time, random\n",
    "\n",
    "import numpy.typing as npt\n",
    "\n",
    "from src.core.sequential import Sequential\n",
    "from src.core.lstm import BiLSTM\n",
    "from src.core.layers import Linear, SigmoidActivation, ReLU, sigmoid\n",
    "from src.core.loss import MSELoss, SigmoidWeightedBCELoss\n",
    "from src.core.optimizers import Adam\n",
    "from src.core.cosine_scheduler import CosineScheduler\n",
    "from src.projects.noise_reduction.fourier_transform import compute_stft_vectorized, compute_stft_inv, convert_to_db, filter_downsample\n",
    "from src.projects.noise_reduction.visualizations import plot_spectrogram, plot_loss_mask, plot_denoising_comparison\n",
    "from src.utils.save_and_load_model import save_model, load_model\n",
    "from src.utils.save_and_load_checkpoint import save_checkpoint, load_checkpoint\n",
    "\n",
    "Tensor = npt.NDArray[cp.float64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a34306",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_path = r\"D:\\Datasets\\Clean and Noisy Audio Dataset 16kHz Spectrogram Magnitudes\\train\\noisy\\p234_002.npy\"\n",
    "clean_path = r\"D:\\Datasets\\Clean and Noisy Audio Dataset 16kHz Spectrogram Magnitudes\\train\\clean\\p234_002.npy\"\n",
    "\n",
    "# We already preprocessed the audios to 16kHz\n",
    "sr, N, hop = 16000, 512, 256\n",
    "\n",
    "n_spec = cp.load(noisy_path)\n",
    "c_spec = cp.load(clean_path)\n",
    "\n",
    "noisy_db = convert_to_db(n_spec, N, is_raw_magnitude=False).get()\n",
    "clean_db = convert_to_db(c_spec, N, is_raw_magnitude=False).get()\n",
    "\n",
    "plot_loss_mask(noisy_db, clean_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e36a780",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDataset:\n",
    "    def __init__(self, path, duration, sr, N, hop, batch_size):\n",
    "        self.duration = duration\n",
    "        self.sr = sr\n",
    "        self.N = N\n",
    "        self.hop = hop\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_split = []\n",
    "        self.test_split = []\n",
    "        \n",
    "        self.num_batches = {\n",
    "            \"train\" : None,\n",
    "            \"test\" : None\n",
    "        }\n",
    "        \n",
    "        for split in ['train', 'test']:\n",
    "            clean_dir_path = os.path.join(path, split, 'clean')\n",
    "            noisy_dir_path = os.path.join(path, split, 'noisy')\n",
    "            \n",
    "            # We know the names are the same in both directories, same amount of files\n",
    "            clean_files = sorted(os.listdir(clean_dir_path))\n",
    "            noisy_files = sorted(os.listdir(noisy_dir_path))\n",
    "            \n",
    "            self.num_batches[split] = len(clean_files) // batch_size\n",
    "            \n",
    "            for batch_index in range(self.num_batches[split]):\n",
    "                start = batch_index * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                batch_paths = []\n",
    "                for i in range(start, end):\n",
    "                    c_path = os.path.join(clean_dir_path, clean_files[i])\n",
    "                    n_path = os.path.join(noisy_dir_path, noisy_files[i])\n",
    "                    \n",
    "                    batch_paths.append((c_path, n_path))\n",
    "                    \n",
    "                if split == \"train\":\n",
    "                    self.train_split.append(batch_paths)\n",
    "                elif split == 'test':\n",
    "                    self.test_split.append(batch_paths)\n",
    "                    \n",
    "    def shuffle_train_data(self):\n",
    "        all_training_paths = []\n",
    "        \n",
    "        for batch_index in range(self.num_batches['train']):\n",
    "            all_training_paths.extend(self.train_split[batch_index])\n",
    "            \n",
    "        self.train_split = []\n",
    "            \n",
    "        random.shuffle(all_training_paths)\n",
    "        \n",
    "        for batch_index in range(self.num_batches['train']):\n",
    "            start = batch_index * self.batch_size\n",
    "            end = start + self.batch_size\n",
    "            \n",
    "            batch_paths = []\n",
    "            \n",
    "            for i in range(start, end):\n",
    "                c_path, n_path = all_training_paths[i]\n",
    "                \n",
    "                batch_paths.append((c_path, n_path))\n",
    "                \n",
    "            self.train_split.append(batch_paths)\n",
    "                        \n",
    "                \n",
    "                    \n",
    "    def get_batch(self, split='train', index=0):\n",
    "        if split == 'train':\n",
    "            batch_list = self.train_split[index]\n",
    "        elif split == 'test':\n",
    "            batch_list = self.test_split[index]\n",
    "            \n",
    "        X_batch, Y_batch, db_noisy_batch, db_clean_batch = [], [], [], [] # LM = Loss Mask\n",
    "            \n",
    "        try:\n",
    "            for clean_path, noisy_path in batch_list:\n",
    "                n_mag = cp.load(noisy_path)\n",
    "                c_mag = cp.load(clean_path) \n",
    "                \n",
    "                db_noisy = convert_to_db(n_mag, self.N, is_raw_magnitude=True)\n",
    "                db_clean = convert_to_db(c_mag, self.N, is_raw_magnitude=True)\n",
    "                \n",
    "                # x_new = (x - center) / radius where center is (A + B) / 2 (center of original distribution, we center data around 0) and radius is (B - A) / 2 (how much the data is off 0 at most) \n",
    "                # x_input = (x_input - (-100 + 0) / 2) / ((0 - (-100)) / 2) # bc our [A, B] is [-100, 0]\n",
    "                x_input = (db_noisy + 40) / 40 # Same as line above\n",
    "                x_input = cp.clip(x_input, -1.0, 1.0, out=x_input) # We clip if the magnitude in convert_to_db was bigger than 1\n",
    "\n",
    "                mask = c_mag / (n_mag + 1e-9) # target_mask = cp.abs(clean) / cp.abs(noisy) -> this means \"how much of clean signal is in the noisy?\" If its 100% then its 1, if 0% then 0.\n",
    "                mask = cp.clip(mask, 0, 1, mask)\n",
    "                \n",
    "                X_batch.append(x_input)\n",
    "                Y_batch.append(mask)\n",
    "                db_noisy_batch.append(db_noisy)\n",
    "                db_clean_batch.append(db_clean)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"\\nCorrupted file found: {clean_path} or {noisy_path}. Delete them and run preproc again.\")\n",
    "            \n",
    "            \n",
    "        X_stack, Y_stack, LM_stack = cp.stack(X_batch), cp.stack(Y_batch), (cp.stack(db_noisy_batch), cp.stack(db_clean_batch))\n",
    "            \n",
    "        del X_batch, Y_batch, db_noisy_batch, db_clean_batch\n",
    "        \n",
    "        return X_stack, Y_stack, LM_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e12fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r\"D:\\Datasets\\Clean and Noisy Audio Dataset 16kHz Spectrogram Magnitudes\"\n",
    "duration = 2 # 2 sec per sample, if less we pad\n",
    "sampling_rate = 16000\n",
    "N, hop = 512, 256\n",
    "batch_size = 128\n",
    "\n",
    "input_dim = N // 2 + 1 # amount of bins\n",
    "hidden_dim = 256 # for BiLSTM its 256 * 2 = 512 theoretically\n",
    "dense_dim = 512\n",
    "output_dim = N // 2 + 1 # also amount of bins (the mask)\n",
    "\n",
    "max_lr = 1e-3\n",
    "min_lr = 1e-5\n",
    "warmup_epochs = 0\n",
    "total_epochs = 50\n",
    "\n",
    "dataset = VoiceDataset(dataset_path, sampling_rate, duration, N, hop, batch_size)\n",
    "\n",
    "model = Sequential([\n",
    "    BiLSTM(input_dim, hidden_dim),\n",
    "    Linear(2 * hidden_dim, output_dim, act='sigmoid'), # Because outputs are returned reshapes, we can apply two times with flattening logic.\n",
    "])\n",
    "\n",
    "loss_fn = SigmoidWeightedBCELoss()\n",
    "\n",
    "params = model.params()\n",
    "optimizer = Adam(params, max_lr, beta1=0.9, beta2=0.999, eps=1e-8)\n",
    "scheduler = CosineScheduler(max_lr, min_lr, warmup_epochs, total_epochs)\n",
    "\n",
    "learnable_layers = {\n",
    "    \"model\" : model\n",
    "}\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d38d37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Batch: 50 | Loss: 0.046690 | BPM: 58.27 batches | Progress: 8.56%\n",
      "Mask max: 0.999709814746138 | Mask min: 0.001999801131042551\n",
      "Epoch: 5 | Batch: 60 | Loss: 0.046579 | BPM: 57.00 batches | Progress: 8.67%\n",
      "Mask max: 0.9996388811123756 | Mask min: 0.0017642780536558365\n",
      "Epoch: 5 | Batch: 70 | Loss: 0.046488 | BPM: 61.24 batches | Progress: 8.78%\n",
      "Mask max: 0.9994009471349978 | Mask min: 0.0012161801512662763\n",
      "Epoch: 5 | Batch: 80 | Loss: 0.046512 | BPM: 58.99 batches | Progress: 8.89%\n",
      "Mask max: 0.999464422916382 | Mask min: 0.001850599278846144\n",
      "Epoch: 5 | Batch: 90 | Loss: 0.046539 | BPM: 58.38 batches | Progress: 9.00%\n",
      "Mask max: 0.9995109140465827 | Mask min: 0.000681699893400152\n",
      "Epoch: 5 | Batch: 100 | Loss: 0.046422 | BPM: 57.10 batches | Progress: 9.11%\n",
      "Mask max: 0.9994155007312068 | Mask min: 0.0015008398625529409\n",
      "Epoch: 5 | Batch: 110 | Loss: 0.046460 | BPM: 49.99 batches | Progress: 9.22%\n",
      "Mask max: 0.9995051855303017 | Mask min: 0.0016712281537454119\n",
      "Epoch: 5 | Batch: 120 | Loss: 0.046429 | BPM: 49.14 batches | Progress: 9.33%\n",
      "Mask max: 0.9995551612495075 | Mask min: 0.0010279614952215447\n",
      "Epoch: 5 | Batch: 130 | Loss: 0.046421 | BPM: 56.32 batches | Progress: 9.44%\n",
      "Mask max: 0.9996048453094426 | Mask min: 0.0011023942290423112\n",
      "Epoch: 5 | Batch: 140 | Loss: 0.046330 | BPM: 55.31 batches | Progress: 9.56%\n",
      "Mask max: 0.9996342713334514 | Mask min: 0.000907352355634359\n",
      "Epoch: 5 | Batch: 150 | Loss: 0.046305 | BPM: 49.72 batches | Progress: 9.67%\n",
      "Mask max: 0.9994617029784174 | Mask min: 0.0017001634530918839\n",
      "Epoch: 5 | Batch: 160 | Loss: 0.046275 | BPM: 38.78 batches | Progress: 9.78%\n",
      "Mask max: 0.9995648797275366 | Mask min: 0.0014545401310541088\n",
      "Epoch: 5 | Batch: 170 | Loss: 0.046297 | BPM: 35.93 batches | Progress: 9.89%\n",
      "Mask max: 0.9996456039774215 | Mask min: 0.0017056610225297851\n",
      "Epoch: 5 | Batch: 180 | Loss: 0.046323 | BPM: 34.68 batches | Progress: 10.00%\n",
      "Mask max: 0.999760746045836 | Mask min: 0.0004171844627321814\n",
      "Finished epoch 5. Training Loss: 0.046323 Test Loss: 0.029242\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 6 | Batch: 1 | Loss: 0.044188 | BPM: 58.81 batches | Progress: 10.01%\n",
      "Mask max: 0.9997442444975762 | Mask min: 0.0016221535966025476\n",
      "Epoch: 6 | Batch: 10 | Loss: 0.045360 | BPM: 59.48 batches | Progress: 10.11%\n",
      "Mask max: 0.9996538293938941 | Mask min: 0.0014683113919944327\n",
      "Epoch: 6 | Batch: 20 | Loss: 0.045180 | BPM: 56.65 batches | Progress: 10.22%\n",
      "Mask max: 0.9996342288582867 | Mask min: 0.001163458937730924\n",
      "Epoch: 6 | Batch: 30 | Loss: 0.045639 | BPM: 50.04 batches | Progress: 10.33%\n",
      "Mask max: 0.999526308299463 | Mask min: 0.0010665962769215156\n",
      "Epoch: 6 | Batch: 40 | Loss: 0.045690 | BPM: 56.92 batches | Progress: 10.44%\n",
      "Mask max: 0.9993140465823048 | Mask min: 0.0009670267146449587\n",
      "Epoch: 6 | Batch: 50 | Loss: 0.045875 | BPM: 54.64 batches | Progress: 10.56%\n",
      "Mask max: 0.9995977354199899 | Mask min: 0.0014572501403496614\n",
      "Epoch: 6 | Batch: 60 | Loss: 0.045790 | BPM: 52.28 batches | Progress: 10.67%\n",
      "Mask max: 0.9993994145331945 | Mask min: 0.0016822506690325942\n",
      "Epoch: 6 | Batch: 70 | Loss: 0.045815 | BPM: 49.45 batches | Progress: 10.78%\n",
      "Mask max: 0.999543142533497 | Mask min: 0.0014410494325351071\n",
      "Epoch: 6 | Batch: 80 | Loss: 0.045678 | BPM: 54.53 batches | Progress: 10.89%\n",
      "Mask max: 0.999654067679568 | Mask min: 0.0010797734259879025\n",
      "Epoch: 6 | Batch: 90 | Loss: 0.045727 | BPM: 54.84 batches | Progress: 11.00%\n",
      "Mask max: 0.9995481039843228 | Mask min: 0.0014612329742631472\n",
      "Epoch: 6 | Batch: 100 | Loss: 0.045658 | BPM: 50.21 batches | Progress: 11.11%\n",
      "Mask max: 0.9996174594277598 | Mask min: 0.0015377196063215152\n",
      "Epoch: 6 | Batch: 110 | Loss: 0.045674 | BPM: 52.31 batches | Progress: 11.22%\n",
      "Mask max: 0.9994204152969851 | Mask min: 0.0011126247474063033\n",
      "Epoch: 6 | Batch: 120 | Loss: 0.045665 | BPM: 50.81 batches | Progress: 11.33%\n",
      "Mask max: 0.9996352622148722 | Mask min: 0.0013398700554329188\n",
      "Epoch: 6 | Batch: 130 | Loss: 0.045728 | BPM: 51.93 batches | Progress: 11.44%\n",
      "Mask max: 0.999750609242322 | Mask min: 0.0010985599867280908\n",
      "Epoch: 6 | Batch: 140 | Loss: 0.045622 | BPM: 44.47 batches | Progress: 11.56%\n",
      "Mask max: 0.999730634173414 | Mask min: 0.0008298148738749711\n",
      "Epoch: 6 | Batch: 150 | Loss: 0.045635 | BPM: 42.65 batches | Progress: 11.67%\n",
      "Mask max: 0.9996691000514834 | Mask min: 0.00105321434789384\n",
      "Epoch: 6 | Batch: 160 | Loss: 0.045635 | BPM: 37.98 batches | Progress: 11.78%\n",
      "Mask max: 0.999629140094306 | Mask min: 0.0009455266294897933\n",
      "Epoch: 6 | Batch: 170 | Loss: 0.045673 | BPM: 32.20 batches | Progress: 11.89%\n",
      "Mask max: 0.9997188294845263 | Mask min: 0.0008615339630643746\n",
      "Epoch: 6 | Batch: 180 | Loss: 0.045618 | BPM: 33.72 batches | Progress: 12.00%\n",
      "Mask max: 0.9995739090973446 | Mask min: 0.0010188848463427518\n",
      "Finished epoch 6. Training Loss: 0.045618 Test Loss: 0.028696\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 7 | Batch: 1 | Loss: 0.046725 | BPM: 56.44 batches | Progress: 12.01%\n",
      "Mask max: 0.9995310959030345 | Mask min: 0.0008600891670246589\n",
      "Epoch: 7 | Batch: 10 | Loss: 0.044786 | BPM: 59.27 batches | Progress: 12.11%\n",
      "Mask max: 0.9994067226490558 | Mask min: 0.001227346677838478\n",
      "Epoch: 7 | Batch: 20 | Loss: 0.045193 | BPM: 60.59 batches | Progress: 12.22%\n",
      "Mask max: 0.9993512361460689 | Mask min: 0.0011630868479410527\n",
      "Epoch: 7 | Batch: 30 | Loss: 0.045432 | BPM: 59.99 batches | Progress: 12.33%\n",
      "Mask max: 0.9997306074392066 | Mask min: 0.0012240830113825912\n",
      "Epoch: 7 | Batch: 40 | Loss: 0.045336 | BPM: 61.09 batches | Progress: 12.44%\n",
      "Mask max: 0.9994012087231051 | Mask min: 0.0009097705657316886\n",
      "Epoch: 7 | Batch: 50 | Loss: 0.045349 | BPM: 62.09 batches | Progress: 12.56%\n",
      "Mask max: 0.9997084361294869 | Mask min: 0.0008516187680855685\n",
      "Epoch: 7 | Batch: 60 | Loss: 0.045309 | BPM: 52.61 batches | Progress: 12.67%\n",
      "Mask max: 0.9997055068419841 | Mask min: 0.0011783321981129156\n",
      "Epoch: 7 | Batch: 70 | Loss: 0.045317 | BPM: 55.33 batches | Progress: 12.78%\n",
      "Mask max: 0.9996626184356421 | Mask min: 0.0013159666605651764\n",
      "Epoch: 7 | Batch: 80 | Loss: 0.045367 | BPM: 56.57 batches | Progress: 12.89%\n",
      "Mask max: 0.9997818001763783 | Mask min: 0.0008610373273201436\n",
      "Epoch: 7 | Batch: 90 | Loss: 0.045322 | BPM: 55.95 batches | Progress: 13.00%\n",
      "Mask max: 0.9997190760671041 | Mask min: 0.0011722578102380658\n",
      "Epoch: 7 | Batch: 100 | Loss: 0.045340 | BPM: 52.55 batches | Progress: 13.11%\n",
      "Mask max: 0.9994428898682198 | Mask min: 0.0008371688603536958\n",
      "Epoch: 7 | Batch: 110 | Loss: 0.045305 | BPM: 47.90 batches | Progress: 13.22%\n",
      "Mask max: 0.999729816307784 | Mask min: 0.0007675100180835685\n",
      "Epoch: 7 | Batch: 120 | Loss: 0.045189 | BPM: 53.25 batches | Progress: 13.33%\n",
      "Mask max: 0.9998343940751845 | Mask min: 0.0011687062130420784\n",
      "Epoch: 7 | Batch: 130 | Loss: 0.045201 | BPM: 47.83 batches | Progress: 13.44%\n",
      "Mask max: 0.9997933913950996 | Mask min: 0.0011347004991810598\n",
      "Epoch: 7 | Batch: 140 | Loss: 0.045163 | BPM: 45.45 batches | Progress: 13.56%\n",
      "Mask max: 0.9996479835299268 | Mask min: 0.0013040097300868157\n",
      "Epoch: 7 | Batch: 150 | Loss: 0.045153 | BPM: 43.68 batches | Progress: 13.67%\n",
      "Mask max: 0.999740565603376 | Mask min: 0.0008659910357515609\n",
      "Epoch: 7 | Batch: 160 | Loss: 0.045155 | BPM: 38.30 batches | Progress: 13.78%\n",
      "Mask max: 0.9998110644343279 | Mask min: 0.0011606815408440384\n",
      "Epoch: 7 | Batch: 170 | Loss: 0.045122 | BPM: 28.16 batches | Progress: 13.89%\n",
      "Mask max: 0.999623207180944 | Mask min: 0.0007968138582704552\n",
      "Epoch: 7 | Batch: 180 | Loss: 0.045168 | BPM: 32.89 batches | Progress: 14.00%\n",
      "Mask max: 0.999729074806776 | Mask min: 0.0011281063799098885\n",
      "Finished epoch 7. Training Loss: 0.045168 Test Loss: 0.029076\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 8 | Batch: 1 | Loss: 0.043825 | BPM: 63.95 batches | Progress: 14.01%\n",
      "Mask max: 0.9997972981297198 | Mask min: 0.00180379921158977\n",
      "Epoch: 8 | Batch: 10 | Loss: 0.044456 | BPM: 56.77 batches | Progress: 14.11%\n",
      "Mask max: 0.9997588532266796 | Mask min: 0.0009443227087407927\n",
      "Epoch: 8 | Batch: 20 | Loss: 0.044816 | BPM: 61.45 batches | Progress: 14.22%\n",
      "Mask max: 0.9997741615117984 | Mask min: 0.0013544849911058107\n",
      "Epoch: 8 | Batch: 30 | Loss: 0.044259 | BPM: 59.65 batches | Progress: 14.33%\n",
      "Mask max: 0.9997224583182304 | Mask min: 0.0006348668806468929\n",
      "Epoch: 8 | Batch: 40 | Loss: 0.044203 | BPM: 59.70 batches | Progress: 14.44%\n",
      "Mask max: 0.9996719356484697 | Mask min: 0.001020402894909859\n",
      "Epoch: 8 | Batch: 50 | Loss: 0.044361 | BPM: 60.00 batches | Progress: 14.56%\n",
      "Mask max: 0.9996979758126975 | Mask min: 0.001014053639239725\n",
      "Epoch: 8 | Batch: 60 | Loss: 0.044300 | BPM: 55.14 batches | Progress: 14.67%\n",
      "Mask max: 0.9997669744527877 | Mask min: 0.0009728119828544245\n",
      "Epoch: 8 | Batch: 70 | Loss: 0.044499 | BPM: 49.92 batches | Progress: 14.78%\n",
      "Mask max: 0.9997677248249881 | Mask min: 0.000987775925735216\n",
      "Epoch: 8 | Batch: 80 | Loss: 0.044612 | BPM: 52.62 batches | Progress: 14.89%\n",
      "Mask max: 0.9996301056640372 | Mask min: 0.0008941954857602242\n",
      "Epoch: 8 | Batch: 90 | Loss: 0.044540 | BPM: 54.26 batches | Progress: 15.00%\n",
      "Mask max: 0.9998017965397739 | Mask min: 0.0011767311677747477\n",
      "Epoch: 8 | Batch: 100 | Loss: 0.044701 | BPM: 51.53 batches | Progress: 15.11%\n",
      "Mask max: 0.999052622434997 | Mask min: 0.0008484124849247911\n",
      "Epoch: 8 | Batch: 110 | Loss: 0.044767 | BPM: 52.23 batches | Progress: 15.22%\n",
      "Mask max: 0.9996561832104199 | Mask min: 0.0008141288457517064\n",
      "Epoch: 8 | Batch: 120 | Loss: 0.044757 | BPM: 52.37 batches | Progress: 15.33%\n",
      "Mask max: 0.9996074464985385 | Mask min: 0.0014138105980864288\n",
      "Epoch: 8 | Batch: 130 | Loss: 0.044744 | BPM: 53.75 batches | Progress: 15.44%\n",
      "Mask max: 0.999700650925613 | Mask min: 0.0013386957527549506\n",
      "Epoch: 8 | Batch: 140 | Loss: 0.044705 | BPM: 30.33 batches | Progress: 15.56%\n",
      "Mask max: 0.9996359527233277 | Mask min: 0.0011350793334655488\n",
      "Epoch: 8 | Batch: 150 | Loss: 0.044699 | BPM: 25.06 batches | Progress: 15.67%\n",
      "Mask max: 0.9998022458947334 | Mask min: 0.0011491303618864487\n",
      "Epoch: 8 | Batch: 160 | Loss: 0.044711 | BPM: 29.60 batches | Progress: 15.78%\n",
      "Mask max: 0.9996195667982642 | Mask min: 0.0008444604159915284\n",
      "Epoch: 8 | Batch: 170 | Loss: 0.044733 | BPM: 30.14 batches | Progress: 15.89%\n",
      "Mask max: 0.9997938133663946 | Mask min: 0.0019627019538219805\n",
      "Epoch: 8 | Batch: 180 | Loss: 0.044813 | BPM: 28.64 batches | Progress: 16.00%\n",
      "Mask max: 0.9996295395100133 | Mask min: 0.0016416056416890428\n",
      "Finished epoch 8. Training Loss: 0.044813 Test Loss: 0.029007\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 9 | Batch: 1 | Loss: 0.047510 | BPM: 44.28 batches | Progress: 16.01%\n",
      "Mask max: 0.9997252142118809 | Mask min: 0.0011762480917073869\n",
      "Epoch: 9 | Batch: 10 | Loss: 0.044479 | BPM: 40.00 batches | Progress: 16.11%\n",
      "Mask max: 0.999623922034909 | Mask min: 0.0012380682123298284\n",
      "Epoch: 9 | Batch: 20 | Loss: 0.044139 | BPM: 40.15 batches | Progress: 16.22%\n",
      "Mask max: 0.9997851831784913 | Mask min: 0.0007054689307200644\n",
      "Epoch: 9 | Batch: 30 | Loss: 0.044165 | BPM: 41.45 batches | Progress: 16.33%\n",
      "Mask max: 0.9997176408123887 | Mask min: 0.0009225605505219385\n",
      "Epoch: 9 | Batch: 40 | Loss: 0.044164 | BPM: 40.55 batches | Progress: 16.44%\n",
      "Mask max: 0.9997001986816192 | Mask min: 0.0016743038934258785\n",
      "Epoch: 9 | Batch: 50 | Loss: 0.044158 | BPM: 41.83 batches | Progress: 16.56%\n",
      "Mask max: 0.9996991300830222 | Mask min: 0.0011296053632236863\n",
      "Epoch: 9 | Batch: 60 | Loss: 0.044225 | BPM: 44.18 batches | Progress: 16.67%\n",
      "Mask max: 0.9998081181288039 | Mask min: 0.0014106902197359237\n",
      "Epoch: 9 | Batch: 70 | Loss: 0.044137 | BPM: 43.78 batches | Progress: 16.78%\n",
      "Mask max: 0.9997156586555155 | Mask min: 0.0008708809288565853\n",
      "Epoch: 9 | Batch: 80 | Loss: 0.044247 | BPM: 43.26 batches | Progress: 16.89%\n",
      "Mask max: 0.9997587338827302 | Mask min: 0.000776247344938862\n",
      "Epoch: 9 | Batch: 90 | Loss: 0.044429 | BPM: 41.39 batches | Progress: 17.00%\n",
      "Mask max: 0.9997698200698857 | Mask min: 0.0008859583029170073\n",
      "Epoch: 9 | Batch: 100 | Loss: 0.044398 | BPM: 41.53 batches | Progress: 17.11%\n",
      "Mask max: 0.9996686509564336 | Mask min: 0.0016161835439163221\n",
      "Epoch: 9 | Batch: 110 | Loss: 0.044450 | BPM: 45.58 batches | Progress: 17.22%\n",
      "Mask max: 0.9996240050763681 | Mask min: 0.0011161562524101111\n",
      "Epoch: 9 | Batch: 120 | Loss: 0.044366 | BPM: 40.67 batches | Progress: 17.33%\n",
      "Mask max: 0.999678333628324 | Mask min: 0.0008489739805345146\n",
      "Epoch: 9 | Batch: 130 | Loss: 0.044372 | BPM: 44.45 batches | Progress: 17.44%\n",
      "Mask max: 0.9996906918046312 | Mask min: 0.0008597983348235719\n",
      "Epoch: 9 | Batch: 140 | Loss: 0.044453 | BPM: 38.29 batches | Progress: 17.56%\n",
      "Mask max: 0.9997678709314276 | Mask min: 0.0007758202973710435\n",
      "Epoch: 9 | Batch: 150 | Loss: 0.044490 | BPM: 41.64 batches | Progress: 17.67%\n",
      "Mask max: 0.9997791237122768 | Mask min: 0.0012696476138199652\n",
      "Epoch: 9 | Batch: 160 | Loss: 0.044471 | BPM: 38.39 batches | Progress: 17.78%\n",
      "Mask max: 0.9997931120565089 | Mask min: 0.001477427856160026\n",
      "Epoch: 9 | Batch: 170 | Loss: 0.044453 | BPM: 42.60 batches | Progress: 17.89%\n",
      "Mask max: 0.9996640079376362 | Mask min: 0.0012366249468199393\n",
      "Epoch: 9 | Batch: 180 | Loss: 0.044422 | BPM: 41.46 batches | Progress: 18.00%\n",
      "Mask max: 0.9996779295620462 | Mask min: 0.0013062040960319745\n",
      "Finished epoch 9. Training Loss: 0.044422 Test Loss: 0.028470\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 10 | Batch: 1 | Loss: 0.042734 | BPM: 68.09 batches | Progress: 18.01%\n",
      "Mask max: 0.9997509087941752 | Mask min: 0.001172139070257187\n",
      "Epoch: 10 | Batch: 10 | Loss: 0.043839 | BPM: 66.76 batches | Progress: 18.11%\n",
      "Mask max: 0.9997480926351822 | Mask min: 0.0013027316621165572\n",
      "Epoch: 10 | Batch: 20 | Loss: 0.044118 | BPM: 65.91 batches | Progress: 18.22%\n",
      "Mask max: 0.9998203901390119 | Mask min: 0.0007986884779747195\n",
      "Epoch: 10 | Batch: 30 | Loss: 0.044007 | BPM: 65.29 batches | Progress: 18.33%\n",
      "Mask max: 0.9998147969711209 | Mask min: 0.001328087918771633\n",
      "Epoch: 10 | Batch: 40 | Loss: 0.043963 | BPM: 66.93 batches | Progress: 18.44%\n",
      "Mask max: 0.9998001275175398 | Mask min: 0.0009578059621273817\n",
      "Epoch: 10 | Batch: 50 | Loss: 0.043950 | BPM: 65.85 batches | Progress: 18.56%\n",
      "Mask max: 0.9996667158879491 | Mask min: 0.001546742775813024\n",
      "Epoch: 10 | Batch: 60 | Loss: 0.044119 | BPM: 66.49 batches | Progress: 18.67%\n",
      "Mask max: 0.9997345642160338 | Mask min: 0.0011870889819173677\n",
      "Epoch: 10 | Batch: 70 | Loss: 0.044004 | BPM: 57.04 batches | Progress: 18.78%\n",
      "Mask max: 0.9997409436899675 | Mask min: 0.0012775703797247176\n",
      "Epoch: 10 | Batch: 80 | Loss: 0.043927 | BPM: 58.65 batches | Progress: 18.89%\n",
      "Mask max: 0.999854622262737 | Mask min: 0.001233438403881778\n",
      "Epoch: 10 | Batch: 90 | Loss: 0.043933 | BPM: 57.69 batches | Progress: 19.00%\n",
      "Mask max: 0.9998261683853462 | Mask min: 0.0012270041022920991\n",
      "Epoch: 10 | Batch: 100 | Loss: 0.043894 | BPM: 54.36 batches | Progress: 19.11%\n",
      "Mask max: 0.9998164815967225 | Mask min: 0.0008567043448269778\n",
      "Epoch: 10 | Batch: 110 | Loss: 0.043839 | BPM: 49.05 batches | Progress: 19.22%\n",
      "Mask max: 0.9996914747663669 | Mask min: 0.0008360403430465327\n",
      "Epoch: 10 | Batch: 120 | Loss: 0.043835 | BPM: 51.80 batches | Progress: 19.33%\n",
      "Mask max: 0.9997041567305353 | Mask min: 0.0007571967715252363\n",
      "Epoch: 10 | Batch: 130 | Loss: 0.043887 | BPM: 49.22 batches | Progress: 19.44%\n",
      "Mask max: 0.9997055143059489 | Mask min: 0.001065394597852885\n",
      "Epoch: 10 | Batch: 140 | Loss: 0.043932 | BPM: 54.30 batches | Progress: 19.56%\n",
      "Mask max: 0.9996741391580696 | Mask min: 0.0012493436759236747\n",
      "Epoch: 10 | Batch: 150 | Loss: 0.044014 | BPM: 47.41 batches | Progress: 19.67%\n",
      "Mask max: 0.999837019193235 | Mask min: 0.001138351390690347\n",
      "Epoch: 10 | Batch: 160 | Loss: 0.044054 | BPM: 53.58 batches | Progress: 19.78%\n",
      "Mask max: 0.9997737493563728 | Mask min: 0.0010357827176279706\n",
      "Epoch: 10 | Batch: 170 | Loss: 0.044088 | BPM: 41.68 batches | Progress: 19.89%\n",
      "Mask max: 0.9997475218172438 | Mask min: 0.001075258282252393\n",
      "Epoch: 10 | Batch: 180 | Loss: 0.044106 | BPM: 30.00 batches | Progress: 20.00%\n",
      "Mask max: 0.9997383339474664 | Mask min: 0.0016082650654899414\n",
      "Finished epoch 10. Training Loss: 0.044106 Test Loss: 0.028404\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 11 | Batch: 1 | Loss: 0.042805 | BPM: 65.65 batches | Progress: 20.01%\n",
      "Mask max: 0.9997464778076289 | Mask min: 0.001209598835739767\n",
      "Epoch: 11 | Batch: 10 | Loss: 0.043633 | BPM: 65.56 batches | Progress: 20.11%\n",
      "Mask max: 0.9997728322959105 | Mask min: 0.001142298725756754\n",
      "Epoch: 11 | Batch: 20 | Loss: 0.044210 | BPM: 63.97 batches | Progress: 20.22%\n",
      "Mask max: 0.9996290746450551 | Mask min: 0.0009329329353546454\n",
      "Epoch: 11 | Batch: 30 | Loss: 0.044315 | BPM: 60.00 batches | Progress: 20.33%\n",
      "Mask max: 0.9995851805259306 | Mask min: 0.0015646949749330648\n",
      "Epoch: 11 | Batch: 40 | Loss: 0.044011 | BPM: 59.85 batches | Progress: 20.44%\n",
      "Mask max: 0.9998308195751412 | Mask min: 0.0012054286899340718\n",
      "Epoch: 11 | Batch: 50 | Loss: 0.043908 | BPM: 61.20 batches | Progress: 20.56%\n",
      "Mask max: 0.9998079480839285 | Mask min: 0.0009608647808545625\n",
      "Epoch: 11 | Batch: 60 | Loss: 0.043850 | BPM: 54.53 batches | Progress: 20.67%\n",
      "Mask max: 0.999787679251325 | Mask min: 0.000995763573404188\n",
      "Epoch: 11 | Batch: 70 | Loss: 0.043893 | BPM: 60.00 batches | Progress: 20.78%\n",
      "Mask max: 0.9997205744802071 | Mask min: 0.0011408107810895666\n",
      "Epoch: 11 | Batch: 80 | Loss: 0.043766 | BPM: 58.07 batches | Progress: 20.89%\n",
      "Mask max: 0.9997803915124175 | Mask min: 0.0010326794085443248\n",
      "Epoch: 11 | Batch: 90 | Loss: 0.043830 | BPM: 52.73 batches | Progress: 21.00%\n",
      "Mask max: 0.9998017077853782 | Mask min: 0.0009228778106511739\n",
      "Epoch: 11 | Batch: 100 | Loss: 0.043944 | BPM: 56.35 batches | Progress: 21.11%\n",
      "Mask max: 0.9997825519123132 | Mask min: 0.0012208504980309157\n",
      "Epoch: 11 | Batch: 110 | Loss: 0.043951 | BPM: 53.34 batches | Progress: 21.22%\n",
      "Mask max: 0.9997363695685805 | Mask min: 0.0009421269729046353\n",
      "Epoch: 11 | Batch: 120 | Loss: 0.043920 | BPM: 52.20 batches | Progress: 21.33%\n",
      "Mask max: 0.9997838726849358 | Mask min: 0.001389402074961376\n",
      "Epoch: 11 | Batch: 130 | Loss: 0.043852 | BPM: 53.66 batches | Progress: 21.44%\n",
      "Mask max: 0.9998015541756371 | Mask min: 0.0008442096122518198\n",
      "Epoch: 11 | Batch: 140 | Loss: 0.043812 | BPM: 50.50 batches | Progress: 21.56%\n",
      "Mask max: 0.9995234233175517 | Mask min: 0.0009167705021734615\n",
      "Epoch: 11 | Batch: 150 | Loss: 0.043860 | BPM: 50.72 batches | Progress: 21.67%\n",
      "Mask max: 0.9997398474078936 | Mask min: 0.0011236862541743485\n",
      "Epoch: 11 | Batch: 160 | Loss: 0.043886 | BPM: 51.20 batches | Progress: 21.78%\n",
      "Mask max: 0.9998175767710334 | Mask min: 0.001646354672885384\n",
      "Epoch: 11 | Batch: 170 | Loss: 0.043841 | BPM: 53.07 batches | Progress: 21.89%\n",
      "Mask max: 0.9997323170181047 | Mask min: 0.0008769506496514147\n",
      "Epoch: 11 | Batch: 180 | Loss: 0.043827 | BPM: 52.63 batches | Progress: 22.00%\n",
      "Mask max: 0.9996902971038082 | Mask min: 0.0010445940448612844\n",
      "Finished epoch 11. Training Loss: 0.043827 Test Loss: 0.028187\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 12 | Batch: 1 | Loss: 0.043008 | BPM: 64.50 batches | Progress: 22.01%\n",
      "Mask max: 0.999804407101868 | Mask min: 0.0010858031670873847\n",
      "Epoch: 12 | Batch: 10 | Loss: 0.043660 | BPM: 67.43 batches | Progress: 22.11%\n",
      "Mask max: 0.9998276078307169 | Mask min: 0.0006920138803217649\n",
      "Epoch: 12 | Batch: 20 | Loss: 0.043756 | BPM: 66.82 batches | Progress: 22.22%\n",
      "Mask max: 0.9996749322189801 | Mask min: 0.0008165637372379786\n",
      "Epoch: 12 | Batch: 30 | Loss: 0.043886 | BPM: 65.87 batches | Progress: 22.33%\n",
      "Mask max: 0.9996584610717801 | Mask min: 0.0010905286246554052\n",
      "Epoch: 12 | Batch: 40 | Loss: 0.043660 | BPM: 64.88 batches | Progress: 22.44%\n",
      "Mask max: 0.9998025947605073 | Mask min: 0.0010856146560990587\n",
      "Epoch: 12 | Batch: 50 | Loss: 0.043686 | BPM: 66.20 batches | Progress: 22.56%\n",
      "Mask max: 0.9996629928456024 | Mask min: 0.0008324807851714803\n",
      "Epoch: 12 | Batch: 60 | Loss: 0.043836 | BPM: 65.37 batches | Progress: 22.67%\n",
      "Mask max: 0.9997262213630103 | Mask min: 0.000912986702150136\n",
      "Epoch: 12 | Batch: 70 | Loss: 0.043848 | BPM: 65.02 batches | Progress: 22.78%\n",
      "Mask max: 0.9997437904972821 | Mask min: 0.0005798474343210026\n",
      "Epoch: 12 | Batch: 80 | Loss: 0.043848 | BPM: 61.55 batches | Progress: 22.89%\n",
      "Mask max: 0.9997663019155666 | Mask min: 0.0007146027154932828\n",
      "Epoch: 12 | Batch: 90 | Loss: 0.043791 | BPM: 63.28 batches | Progress: 23.00%\n",
      "Mask max: 0.9997904435088747 | Mask min: 0.0008656623349314925\n",
      "Epoch: 12 | Batch: 100 | Loss: 0.043796 | BPM: 61.98 batches | Progress: 23.11%\n",
      "Mask max: 0.9998335073367494 | Mask min: 0.0009933222514217114\n",
      "Epoch: 12 | Batch: 110 | Loss: 0.043801 | BPM: 61.23 batches | Progress: 23.22%\n",
      "Mask max: 0.9998620456193957 | Mask min: 0.0014698544779047334\n",
      "Epoch: 12 | Batch: 120 | Loss: 0.043817 | BPM: 59.41 batches | Progress: 23.33%\n",
      "Mask max: 0.9997933397931831 | Mask min: 0.001014721236524333\n",
      "Epoch: 12 | Batch: 130 | Loss: 0.043713 | BPM: 58.97 batches | Progress: 23.44%\n",
      "Mask max: 0.999785109868239 | Mask min: 0.000763853590459744\n",
      "Epoch: 12 | Batch: 140 | Loss: 0.043716 | BPM: 57.05 batches | Progress: 23.56%\n",
      "Mask max: 0.9997717980927865 | Mask min: 0.0009262414169746418\n",
      "Epoch: 12 | Batch: 150 | Loss: 0.043706 | BPM: 57.70 batches | Progress: 23.67%\n",
      "Mask max: 0.9996774334172054 | Mask min: 0.0012037075313896652\n",
      "Epoch: 12 | Batch: 160 | Loss: 0.043700 | BPM: 59.00 batches | Progress: 23.78%\n",
      "Mask max: 0.9998315873612232 | Mask min: 0.0012105205585949165\n",
      "Epoch: 12 | Batch: 170 | Loss: 0.043669 | BPM: 57.63 batches | Progress: 23.89%\n",
      "Mask max: 0.9998680311463788 | Mask min: 0.0008398414666979357\n",
      "Epoch: 12 | Batch: 180 | Loss: 0.043635 | BPM: 32.61 batches | Progress: 24.00%\n",
      "Mask max: 0.9998358770021614 | Mask min: 0.0012225188242640395\n",
      "Finished epoch 12. Training Loss: 0.043635 Test Loss: 0.028222\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 13 | Batch: 1 | Loss: 0.042931 | BPM: 70.02 batches | Progress: 24.01%\n",
      "Mask max: 0.9998707837665447 | Mask min: 0.0012691848152435396\n",
      "Epoch: 13 | Batch: 10 | Loss: 0.042936 | BPM: 70.10 batches | Progress: 24.11%\n",
      "Mask max: 0.9997267395752284 | Mask min: 0.001318990212061754\n",
      "Epoch: 13 | Batch: 20 | Loss: 0.042976 | BPM: 68.01 batches | Progress: 24.22%\n",
      "Mask max: 0.9998666016963914 | Mask min: 0.0012246780473988913\n",
      "Epoch: 13 | Batch: 30 | Loss: 0.043424 | BPM: 67.60 batches | Progress: 24.33%\n",
      "Mask max: 0.9998636663854769 | Mask min: 0.0007925180974335365\n",
      "Epoch: 13 | Batch: 40 | Loss: 0.043096 | BPM: 64.42 batches | Progress: 24.44%\n",
      "Mask max: 0.9997899956003804 | Mask min: 0.0008035339549472565\n",
      "Epoch: 13 | Batch: 50 | Loss: 0.043343 | BPM: 68.84 batches | Progress: 24.56%\n",
      "Mask max: 0.999885495308502 | Mask min: 0.0013751208522806062\n",
      "Epoch: 13 | Batch: 60 | Loss: 0.043432 | BPM: 65.28 batches | Progress: 24.67%\n",
      "Mask max: 0.9997535251134098 | Mask min: 0.000961742321540431\n",
      "Epoch: 13 | Batch: 70 | Loss: 0.043417 | BPM: 66.69 batches | Progress: 24.78%\n",
      "Mask max: 0.9997588496901684 | Mask min: 0.0009558061411561319\n",
      "Epoch: 13 | Batch: 80 | Loss: 0.043352 | BPM: 66.10 batches | Progress: 24.89%\n",
      "Mask max: 0.9997020220156815 | Mask min: 0.0009896051328581355\n",
      "Epoch: 13 | Batch: 90 | Loss: 0.043407 | BPM: 59.58 batches | Progress: 25.00%\n",
      "Mask max: 0.9997704919644728 | Mask min: 0.0008219910585717729\n",
      "Epoch: 13 | Batch: 100 | Loss: 0.043399 | BPM: 64.18 batches | Progress: 25.11%\n",
      "Mask max: 0.9997295092960535 | Mask min: 0.0016261676768834865\n",
      "Epoch: 13 | Batch: 110 | Loss: 0.043429 | BPM: 64.21 batches | Progress: 25.22%\n",
      "Mask max: 0.9997179047628159 | Mask min: 0.0006096390071485057\n",
      "Epoch: 13 | Batch: 120 | Loss: 0.043442 | BPM: 62.06 batches | Progress: 25.33%\n",
      "Mask max: 0.9997637211885364 | Mask min: 0.000954844858085414\n",
      "Epoch: 13 | Batch: 130 | Loss: 0.043405 | BPM: 57.61 batches | Progress: 25.44%\n",
      "Mask max: 0.9996270684220446 | Mask min: 0.0010820165087034206\n",
      "Epoch: 13 | Batch: 140 | Loss: 0.043372 | BPM: 59.34 batches | Progress: 25.56%\n",
      "Mask max: 0.9996250994545888 | Mask min: 0.0004997262987641398\n",
      "Epoch: 13 | Batch: 150 | Loss: 0.043325 | BPM: 56.23 batches | Progress: 25.67%\n",
      "Mask max: 0.999790761697814 | Mask min: 0.0010443290983444384\n",
      "Epoch: 13 | Batch: 160 | Loss: 0.043299 | BPM: 56.73 batches | Progress: 25.78%\n",
      "Mask max: 0.9997153612890388 | Mask min: 0.001183567149696959\n",
      "Epoch: 13 | Batch: 170 | Loss: 0.043384 | BPM: 56.79 batches | Progress: 25.89%\n",
      "Mask max: 0.9996635259139013 | Mask min: 0.0011984271317524418\n",
      "Epoch: 13 | Batch: 180 | Loss: 0.043410 | BPM: 56.89 batches | Progress: 26.00%\n",
      "Mask max: 0.9995838191601442 | Mask min: 0.0012705012548743385\n",
      "Finished epoch 13. Training Loss: 0.043410 Test Loss: 0.028168\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 14 | Batch: 1 | Loss: 0.043543 | BPM: 66.87 batches | Progress: 26.01%\n",
      "Mask max: 0.9997509254866008 | Mask min: 0.0010316002764832806\n",
      "Epoch: 14 | Batch: 10 | Loss: 0.043285 | BPM: 66.89 batches | Progress: 26.11%\n",
      "Mask max: 0.9998046817268342 | Mask min: 0.0013619639984362906\n",
      "Epoch: 14 | Batch: 20 | Loss: 0.043658 | BPM: 66.21 batches | Progress: 26.22%\n",
      "Mask max: 0.9998080741131276 | Mask min: 0.0005984896989277901\n",
      "Epoch: 14 | Batch: 30 | Loss: 0.043248 | BPM: 66.38 batches | Progress: 26.33%\n",
      "Mask max: 0.9998357174928839 | Mask min: 0.0010069362956509407\n",
      "Epoch: 14 | Batch: 40 | Loss: 0.043015 | BPM: 64.64 batches | Progress: 26.44%\n",
      "Mask max: 0.9997814265065954 | Mask min: 0.0010523011427674251\n",
      "Epoch: 14 | Batch: 50 | Loss: 0.042867 | BPM: 62.79 batches | Progress: 26.56%\n",
      "Mask max: 0.9998326075304335 | Mask min: 0.0008150883261674173\n",
      "Epoch: 14 | Batch: 60 | Loss: 0.043002 | BPM: 64.11 batches | Progress: 26.67%\n",
      "Mask max: 0.999889809613416 | Mask min: 0.0010670510159518614\n",
      "Epoch: 14 | Batch: 70 | Loss: 0.042836 | BPM: 65.75 batches | Progress: 26.78%\n",
      "Mask max: 0.9997849608172232 | Mask min: 0.00044490554827850923\n",
      "Epoch: 14 | Batch: 80 | Loss: 0.042824 | BPM: 61.81 batches | Progress: 26.89%\n",
      "Mask max: 0.9997794466148369 | Mask min: 0.0009288435847785857\n",
      "Epoch: 14 | Batch: 90 | Loss: 0.042914 | BPM: 64.71 batches | Progress: 27.00%\n",
      "Mask max: 0.9997885097891636 | Mask min: 0.001023211898120809\n",
      "Epoch: 14 | Batch: 100 | Loss: 0.042930 | BPM: 58.52 batches | Progress: 27.11%\n",
      "Mask max: 0.9998339127429373 | Mask min: 0.0009020216978694654\n",
      "Epoch: 14 | Batch: 110 | Loss: 0.043019 | BPM: 58.22 batches | Progress: 27.22%\n",
      "Mask max: 0.99979980407801 | Mask min: 0.0013807090463352312\n",
      "Epoch: 14 | Batch: 120 | Loss: 0.043093 | BPM: 53.48 batches | Progress: 27.33%\n",
      "Mask max: 0.9998582885605867 | Mask min: 0.0010318045921779411\n",
      "Epoch: 14 | Batch: 130 | Loss: 0.043136 | BPM: 51.82 batches | Progress: 27.44%\n",
      "Mask max: 0.9997771392982607 | Mask min: 0.0011178131079853948\n",
      "Epoch: 14 | Batch: 140 | Loss: 0.043167 | BPM: 56.99 batches | Progress: 27.56%\n",
      "Mask max: 0.9997608860435799 | Mask min: 0.000940655934110019\n",
      "Epoch: 14 | Batch: 150 | Loss: 0.043137 | BPM: 57.62 batches | Progress: 27.67%\n",
      "Mask max: 0.9998013119772798 | Mask min: 0.00133266461879514\n",
      "Epoch: 14 | Batch: 160 | Loss: 0.043168 | BPM: 55.90 batches | Progress: 27.78%\n",
      "Mask max: 0.999812618338838 | Mask min: 0.0014258227525281463\n",
      "Epoch: 14 | Batch: 170 | Loss: 0.043164 | BPM: 55.87 batches | Progress: 27.89%\n",
      "Mask max: 0.9997927859040736 | Mask min: 0.0014838704764336072\n",
      "Epoch: 14 | Batch: 180 | Loss: 0.043183 | BPM: 56.26 batches | Progress: 28.00%\n",
      "Mask max: 0.9998045458074057 | Mask min: 0.0015085544181899354\n",
      "Finished epoch 14. Training Loss: 0.043183 Test Loss: 0.028021\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 15 | Batch: 1 | Loss: 0.043686 | BPM: 70.00 batches | Progress: 28.01%\n",
      "Mask max: 0.9997436612149571 | Mask min: 0.0012162436197721152\n",
      "Epoch: 15 | Batch: 10 | Loss: 0.042945 | BPM: 68.87 batches | Progress: 28.11%\n",
      "Mask max: 0.999746321149432 | Mask min: 0.0018303747928854544\n",
      "Epoch: 15 | Batch: 20 | Loss: 0.043243 | BPM: 67.69 batches | Progress: 28.22%\n",
      "Mask max: 0.9998595880436565 | Mask min: 0.0014223656593151494\n",
      "Epoch: 15 | Batch: 30 | Loss: 0.043079 | BPM: 64.21 batches | Progress: 28.33%\n",
      "Mask max: 0.9997471750000678 | Mask min: 0.0006385893353031332\n",
      "Epoch: 15 | Batch: 40 | Loss: 0.043009 | BPM: 59.64 batches | Progress: 28.44%\n",
      "Mask max: 0.9998935307100927 | Mask min: 0.0008622147669232776\n",
      "Epoch: 15 | Batch: 50 | Loss: 0.043187 | BPM: 60.25 batches | Progress: 28.56%\n",
      "Mask max: 0.999801283811075 | Mask min: 0.001516834904852214\n",
      "Epoch: 15 | Batch: 60 | Loss: 0.043133 | BPM: 59.57 batches | Progress: 28.67%\n",
      "Mask max: 0.9997701932603463 | Mask min: 0.0006615568116250635\n",
      "Epoch: 15 | Batch: 70 | Loss: 0.043220 | BPM: 54.62 batches | Progress: 28.78%\n",
      "Mask max: 0.9997970132624966 | Mask min: 0.0010366021236301983\n",
      "Epoch: 15 | Batch: 80 | Loss: 0.043295 | BPM: 58.30 batches | Progress: 28.89%\n",
      "Mask max: 0.9997415502107292 | Mask min: 0.0010557984311191622\n",
      "Epoch: 15 | Batch: 90 | Loss: 0.043229 | BPM: 59.40 batches | Progress: 29.00%\n",
      "Mask max: 0.9997755111336263 | Mask min: 0.00034070441575513247\n",
      "Epoch: 15 | Batch: 100 | Loss: 0.043241 | BPM: 53.83 batches | Progress: 29.11%\n",
      "Mask max: 0.9997713988331239 | Mask min: 0.001425821918981602\n",
      "Epoch: 15 | Batch: 110 | Loss: 0.043173 | BPM: 60.25 batches | Progress: 29.22%\n",
      "Mask max: 0.9997187650023748 | Mask min: 0.0008048808051368755\n",
      "Epoch: 15 | Batch: 120 | Loss: 0.043221 | BPM: 58.59 batches | Progress: 29.33%\n",
      "Mask max: 0.9997037408326477 | Mask min: 0.0006791159805580631\n",
      "Epoch: 15 | Batch: 130 | Loss: 0.043157 | BPM: 57.55 batches | Progress: 29.44%\n",
      "Mask max: 0.9997644033076643 | Mask min: 0.0010284357938996538\n",
      "Epoch: 15 | Batch: 140 | Loss: 0.043160 | BPM: 56.29 batches | Progress: 29.56%\n",
      "Mask max: 0.9998336483866312 | Mask min: 0.0007812732325362953\n",
      "Epoch: 15 | Batch: 150 | Loss: 0.043172 | BPM: 54.18 batches | Progress: 29.67%\n",
      "Mask max: 0.9997846786089059 | Mask min: 0.0007517857616697487\n",
      "Epoch: 15 | Batch: 160 | Loss: 0.043084 | BPM: 58.38 batches | Progress: 29.78%\n",
      "Mask max: 0.9998369536484775 | Mask min: 0.0009585527592660774\n",
      "Epoch: 15 | Batch: 170 | Loss: 0.043038 | BPM: 51.47 batches | Progress: 29.89%\n",
      "Mask max: 0.9997538272746924 | Mask min: 0.001007683287440141\n",
      "Epoch: 15 | Batch: 180 | Loss: 0.043070 | BPM: 54.88 batches | Progress: 30.00%\n",
      "Mask max: 0.9997762494838486 | Mask min: 0.0010220151874530572\n",
      "Finished epoch 15. Training Loss: 0.043070 Test Loss: 0.027993\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 16 | Batch: 1 | Loss: 0.041769 | BPM: 70.42 batches | Progress: 30.01%\n",
      "Mask max: 0.9997431583444589 | Mask min: 0.0007431907399351291\n",
      "Epoch: 16 | Batch: 10 | Loss: 0.043854 | BPM: 70.67 batches | Progress: 30.11%\n",
      "Mask max: 0.9998587127158385 | Mask min: 0.0010101785738547502\n",
      "Epoch: 16 | Batch: 20 | Loss: 0.043152 | BPM: 68.57 batches | Progress: 30.22%\n",
      "Mask max: 0.9998738985947496 | Mask min: 0.0019400808867524166\n",
      "Epoch: 16 | Batch: 30 | Loss: 0.042676 | BPM: 60.72 batches | Progress: 30.33%\n",
      "Mask max: 0.9998654407790903 | Mask min: 0.0006372014092176827\n",
      "Epoch: 16 | Batch: 40 | Loss: 0.042715 | BPM: 69.31 batches | Progress: 30.44%\n",
      "Mask max: 0.9997670120989679 | Mask min: 0.0008820627812823202\n",
      "Epoch: 16 | Batch: 50 | Loss: 0.042776 | BPM: 63.05 batches | Progress: 30.56%\n",
      "Mask max: 0.999842371509633 | Mask min: 0.0007866610116639048\n",
      "Epoch: 16 | Batch: 60 | Loss: 0.043025 | BPM: 65.00 batches | Progress: 30.67%\n",
      "Mask max: 0.9998796365470819 | Mask min: 0.0012959215514072517\n",
      "Epoch: 16 | Batch: 70 | Loss: 0.043057 | BPM: 62.01 batches | Progress: 30.78%\n",
      "Mask max: 0.9997118558481951 | Mask min: 0.0004519874888771654\n",
      "Epoch: 16 | Batch: 80 | Loss: 0.043005 | BPM: 64.59 batches | Progress: 30.89%\n",
      "Mask max: 0.9998545686169757 | Mask min: 0.001297372613502124\n",
      "Epoch: 16 | Batch: 90 | Loss: 0.042985 | BPM: 61.97 batches | Progress: 31.00%\n",
      "Mask max: 0.9997879179151155 | Mask min: 0.0006048393762678305\n",
      "Epoch: 16 | Batch: 100 | Loss: 0.043014 | BPM: 62.95 batches | Progress: 31.11%\n",
      "Mask max: 0.9998182780018899 | Mask min: 0.0010426210519048444\n",
      "Epoch: 16 | Batch: 110 | Loss: 0.043067 | BPM: 62.14 batches | Progress: 31.22%\n",
      "Mask max: 0.9998135922878102 | Mask min: 0.0007806366567214107\n",
      "Epoch: 16 | Batch: 120 | Loss: 0.042935 | BPM: 60.23 batches | Progress: 31.33%\n",
      "Mask max: 0.9998605479766633 | Mask min: 0.0012281035750942991\n",
      "Epoch: 16 | Batch: 130 | Loss: 0.042878 | BPM: 58.48 batches | Progress: 31.44%\n",
      "Mask max: 0.999890011478962 | Mask min: 0.0006627794686574236\n",
      "Epoch: 16 | Batch: 140 | Loss: 0.042854 | BPM: 58.76 batches | Progress: 31.56%\n",
      "Mask max: 0.9998637497710562 | Mask min: 0.0011976843287377294\n",
      "Epoch: 16 | Batch: 150 | Loss: 0.042886 | BPM: 58.48 batches | Progress: 31.67%\n",
      "Mask max: 0.999817434006665 | Mask min: 0.0008986634394824366\n",
      "Epoch: 16 | Batch: 160 | Loss: 0.042850 | BPM: 57.88 batches | Progress: 31.78%\n",
      "Mask max: 0.9997598136769307 | Mask min: 0.0006947931335812031\n",
      "Epoch: 16 | Batch: 170 | Loss: 0.042861 | BPM: 53.55 batches | Progress: 31.89%\n",
      "Mask max: 0.9998942321436899 | Mask min: 0.0006111395888574641\n",
      "Epoch: 16 | Batch: 180 | Loss: 0.042860 | BPM: 33.76 batches | Progress: 32.00%\n",
      "Mask max: 0.9998884177239417 | Mask min: 0.0012063122783885537\n",
      "Finished epoch 16. Training Loss: 0.042860 Test Loss: 0.028167\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 17 | Batch: 1 | Loss: 0.044638 | BPM: 65.78 batches | Progress: 32.01%\n",
      "Mask max: 0.9998837395448782 | Mask min: 0.0010280392859931336\n",
      "Epoch: 17 | Batch: 10 | Loss: 0.042779 | BPM: 68.49 batches | Progress: 32.11%\n",
      "Mask max: 0.9998118370975891 | Mask min: 0.0009298086652201448\n",
      "Epoch: 17 | Batch: 20 | Loss: 0.042674 | BPM: 66.62 batches | Progress: 32.22%\n",
      "Mask max: 0.9998174235217453 | Mask min: 0.0007988509001748953\n",
      "Epoch: 17 | Batch: 30 | Loss: 0.042433 | BPM: 63.72 batches | Progress: 32.33%\n",
      "Mask max: 0.9998060906836082 | Mask min: 0.0006056052011539411\n",
      "Epoch: 17 | Batch: 40 | Loss: 0.042591 | BPM: 65.95 batches | Progress: 32.44%\n",
      "Mask max: 0.9999047874257951 | Mask min: 0.0004703995776480316\n",
      "Epoch: 17 | Batch: 50 | Loss: 0.042836 | BPM: 67.04 batches | Progress: 32.56%\n",
      "Mask max: 0.9998152108352774 | Mask min: 0.0007934202245065158\n",
      "Epoch: 17 | Batch: 60 | Loss: 0.042982 | BPM: 64.56 batches | Progress: 32.67%\n",
      "Mask max: 0.999813638595971 | Mask min: 0.0010871877920628575\n",
      "Epoch: 17 | Batch: 70 | Loss: 0.042840 | BPM: 63.86 batches | Progress: 32.78%\n",
      "Mask max: 0.9997625132351595 | Mask min: 0.0005615284672118224\n",
      "Epoch: 17 | Batch: 80 | Loss: 0.042872 | BPM: 58.86 batches | Progress: 32.89%\n",
      "Mask max: 0.9998089779252476 | Mask min: 0.0010532126053902052\n",
      "Epoch: 17 | Batch: 90 | Loss: 0.042668 | BPM: 62.26 batches | Progress: 33.00%\n",
      "Mask max: 0.9997796916070828 | Mask min: 0.0008704759831372861\n",
      "Epoch: 17 | Batch: 100 | Loss: 0.042667 | BPM: 60.43 batches | Progress: 33.11%\n",
      "Mask max: 0.9998980750575259 | Mask min: 0.001263662983529843\n",
      "Epoch: 17 | Batch: 110 | Loss: 0.042662 | BPM: 60.23 batches | Progress: 33.22%\n",
      "Mask max: 0.999792068658439 | Mask min: 0.000795033541708186\n",
      "Epoch: 17 | Batch: 120 | Loss: 0.042665 | BPM: 57.41 batches | Progress: 33.33%\n",
      "Mask max: 0.999832061430226 | Mask min: 0.0009153948610997689\n",
      "Epoch: 17 | Batch: 130 | Loss: 0.042699 | BPM: 56.28 batches | Progress: 33.44%\n",
      "Mask max: 0.9998777413727828 | Mask min: 0.0009508526292156538\n",
      "Epoch: 17 | Batch: 140 | Loss: 0.042689 | BPM: 54.59 batches | Progress: 33.56%\n",
      "Mask max: 0.9998921192150164 | Mask min: 0.0004591725267001584\n",
      "Epoch: 17 | Batch: 150 | Loss: 0.042707 | BPM: 52.25 batches | Progress: 33.67%\n",
      "Mask max: 0.9998791950123815 | Mask min: 0.0008463146732238384\n",
      "Epoch: 17 | Batch: 160 | Loss: 0.042697 | BPM: 55.51 batches | Progress: 33.78%\n",
      "Mask max: 0.9998726621052548 | Mask min: 0.0007110810253092015\n",
      "Epoch: 17 | Batch: 170 | Loss: 0.042681 | BPM: 58.36 batches | Progress: 33.89%\n",
      "Mask max: 0.9998162904484618 | Mask min: 0.000992633935519165\n",
      "Epoch: 17 | Batch: 180 | Loss: 0.042705 | BPM: 55.74 batches | Progress: 34.00%\n",
      "Mask max: 0.9998026854354419 | Mask min: 0.0006237490064324413\n",
      "Finished epoch 17. Training Loss: 0.042705 Test Loss: 0.027515\n",
      "Saved checkpoint to: models/best_denoise.npz\n",
      "Epoch: 18 | Batch: 1 | Loss: 0.044552 | BPM: 60.59 batches | Progress: 34.01%\n",
      "Mask max: 0.9998559301831964 | Mask min: 0.0010285563268185708\n",
      "Epoch: 18 | Batch: 10 | Loss: 0.042088 | BPM: 62.05 batches | Progress: 34.11%\n",
      "Mask max: 0.9998470721638334 | Mask min: 0.0009315163000240973\n",
      "Epoch: 18 | Batch: 20 | Loss: 0.042683 | BPM: 62.18 batches | Progress: 34.22%\n",
      "Mask max: 0.9998693527948095 | Mask min: 0.0013217465691811225\n",
      "Epoch: 18 | Batch: 30 | Loss: 0.042760 | BPM: 65.14 batches | Progress: 34.33%\n",
      "Mask max: 0.9998435501565266 | Mask min: 0.001091841915374801\n",
      "Epoch: 18 | Batch: 40 | Loss: 0.042706 | BPM: 67.27 batches | Progress: 34.44%\n",
      "Mask max: 0.999847675982956 | Mask min: 0.0011406021458030774\n",
      "Epoch: 18 | Batch: 50 | Loss: 0.042792 | BPM: 61.87 batches | Progress: 34.56%\n",
      "Mask max: 0.9998359714714994 | Mask min: 0.000565949399428432\n",
      "Epoch: 18 | Batch: 60 | Loss: 0.042730 | BPM: 64.07 batches | Progress: 34.67%\n",
      "Mask max: 0.9998393897279245 | Mask min: 0.0009033028641055576\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = 'models/best_denoise.npz'\n",
    "resume_training = False\n",
    "\n",
    "start_epoch = 1\n",
    "best_loss = float('inf')\n",
    "if resume_training:\n",
    "    start_epoch, best_loss = load_checkpoint(checkpoint_path, learnable_layers, optimizer)\n",
    "    \n",
    "train_loss_history = []\n",
    "test_loss_history = []\n",
    "\n",
    "for epoch in range(start_epoch, total_epochs+1):\n",
    "    learning_rate = scheduler.get_lr(epoch)\n",
    "    \n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    \n",
    "    # Train\n",
    "    for batch_index in range(dataset.num_batches['train']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        X_batch, Y_batch, (db_noisy, db_clean) = dataset.get_batch('train', batch_index)\n",
    "        \n",
    "        speech_to_keep = (db_clean + 80) / 80 # scale to [0, 1]\n",
    "        speech_to_keep = cp.clip(speech_to_keep, 0, 1) # How much we want to keep it\n",
    "        noise_to_delete = cp.clip(db_noisy - db_clean, 0, 80) / 80 # How much we want to zero it out\n",
    "        \n",
    "        loss_mask = cp.maximum(speech_to_keep, noise_to_delete) # take max of masks\n",
    "\n",
    "        logits = model.forward(X_batch)\n",
    "        loss, mask = loss_fn.forward(logits, Y_batch, loss_mask)\n",
    "        \n",
    "        epoch_train_losses.append(loss.get())\n",
    "        \n",
    "        dlogits = loss_fn.backward()\n",
    "        dinput = model.backward(dlogits)\n",
    "        \n",
    "        optimizer.step(learning_rate)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        diff = end_time - start_time\n",
    "        \n",
    "        if (batch_index + 1) % 10 == 0 or batch_index == 0: # Print the first batch too\n",
    "            total_steps = total_epochs * dataset.num_batches['train']\n",
    "            completed_steps = ((epoch - 1) * dataset.num_batches['train']) + (batch_index + 1) \n",
    "            progress = completed_steps / total_steps * 100\n",
    "            print(f\"Epoch: {epoch} | Batch: {batch_index+1} | Loss: {np.mean(epoch_train_losses):.6f} | BPM: {60 / diff:.2f} batches | Progress: {progress:.2f}%\")\n",
    "            print(f\"Mask max: {cp.max(mask)} | Mask min: {cp.min(mask)}\")\n",
    "    \n",
    "    # Eval\n",
    "    for batch_index in range(dataset.num_batches['test']):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        X_batch, Y_batch, (db_noisy, db_clean) = dataset.get_batch('test', batch_index)\n",
    "        \n",
    "        speech_to_keep = (db_clean + 80) / 80 # same as in training loop\n",
    "        speech_to_keep = cp.clip(speech_to_keep, 0, 1) \n",
    "        noise_to_delete = cp.clip(db_noisy - db_clean, 0, 80) / 80 \n",
    "        \n",
    "        loss_mask = cp.maximum(speech_to_keep, noise_to_delete) \n",
    "\n",
    "        logits = model.forward(X_batch)\n",
    "        loss, mask = loss_fn.forward(logits, Y_batch, loss_mask)\n",
    "        \n",
    "        epoch_test_losses.append(loss.get())\n",
    "    \n",
    "    ave_train_loss = np.mean(epoch_train_losses)\n",
    "    ave_test_loss = np.mean(epoch_test_losses)\n",
    "    \n",
    "    train_loss_history.append(ave_train_loss) # already Numpy\n",
    "    test_loss_history.append(ave_test_loss) # same\n",
    "    \n",
    "    print(f\"Finished epoch {epoch}. Training Loss: {np.mean(ave_train_loss):.6f} Test Loss: {np.mean(ave_test_loss):.6f}\")\n",
    "    \n",
    "    if ave_train_loss < best_loss:\n",
    "        best_loss = ave_train_loss\n",
    "        save_checkpoint(checkpoint_path, learnable_layers, optimizer, epoch, best_loss) \n",
    "        \n",
    "    dataset.shuffle_train_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65caafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16 ,8))\n",
    "\n",
    "plt.plot(train_loss_history, label='Training Loss')\n",
    "plt.plot(test_loss_history, label='Test Loss')\n",
    "\n",
    "plt.title('Training vs Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_recording(audio: Tensor, N: int, hop: int, model: Sequential):\n",
    "\n",
    "    spec, orig_len = compute_stft_vectorized(audio, N, hop) # (num_frames, K), we need orig_len to reverse the padding\n",
    "    mag = convert_to_db(spec, N, is_raw_magnitude=False) # (num_frames, K)\n",
    "    \n",
    "    mag_norm = (mag + 40) / 40 # Same normalization as training as data ranges between (-80, 0)\n",
    "    mag_norm = cp.clip(mag_norm, -1.0, 1.0) # safety clip\n",
    "\n",
    "    logits = model.forward(mag_norm[cp.newaxis, :, :]).squeeze() # add new axis for B=1\n",
    "    mask = sigmoid(logits)\n",
    "    \n",
    "    cleaned_spec = spec * mask # scales imaginary and real components accordingly (just the magnitude)\n",
    "    audio = compute_stft_inv(cleaned_spec, orig_len, N, hop)\n",
    "    \n",
    "    return audio, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf024e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = \"models/best_denoise.npz\"\n",
    "load_model(best_model_path, learnable_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afbeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_audio_path = r\"noisy_sample.wav\"\n",
    "output_audio_path = r\"clean_sample.wav\"\n",
    "\n",
    "noisy_audio, _ = lb.load(noisy_audio_path, sr=sampling_rate)\n",
    "clean_audio, mask = clean_recording(noisy_audio, N, hop, model)\n",
    "\n",
    "n_spec, _ = compute_stft_vectorized(noisy_audio, N, hop) # both are in the complex plane\n",
    "c_spec, _ = compute_stft_vectorized(clean_audio, N, hop)\n",
    "\n",
    "plot_denoising_comparison(n_spec.get(), c_spec.get(), mask.get(), sampling_rate, N, hop)\n",
    "\n",
    "sf.write(output_audio_path, clean_audio.get(), sampling_rate)\n",
    "print(f\"Saved audio to: {output_audio_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
